{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 12:29:23 WARN Utils: Your hostname, Davis-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)\n",
      "23/03/19 12:29:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 12:29:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/03/19 12:29:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setMaster(\"local[*]\")\n",
    "    .setAppName(\"model_cab_prices\")\n",
    "    .set(\"spark.executor.memory\", \"4g\")\n",
    "    .set(\"spark.executor.cores\", \"5\")\n",
    ")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"../Uber_Lyft_Cab_prices/df_final.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = (\n",
    "    df.select(\n",
    "    \"distance\", \"cab_type\", \"destination\", \"source\", \"price\",\n",
    "    \"surge_multiplier\", \"product_id\", \"hour\", \"day_of_week\",\n",
    "    \"month\", \"temp\", \"clouds\", \"pressure\", \"rain\", \"humidity\", \"wind\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainData, testData) = df_final.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_transform = [\"cab_type\", \"destination\", \"source\", \"product_id\", \"day_of_week\"]\n",
    "tokenizer = [Tokenizer(inputCol=col, outputCol=col+\"_token\") for col in cols_to_transform]\n",
    "hashingTF = [HashingTF(inputCol=col+\"_token\", outputCol=col+\"_hash\") for col in cols_to_transform]\n",
    "pipeline = [Pipeline(stages=[tokenizer[i], hashingTF[i]]) for i in range(len(cols_to_transform))]\n",
    "for i in range(len(cols_to_transform)):\n",
    "    trainData = pipeline[i].fit(trainData).transform(trainData)\n",
    "    testData = pipeline[i].fit(testData).transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [\n",
    "    \"cab_type_hash\", \"destination_hash\", \"source_hash\",\n",
    "    \"product_id_hash\", \"day_of_week_hash\", \"distance\",\n",
    "    \"surge_multiplier\", \"hour\", \"month\", \"temp\", \"clouds\",\n",
    "    \"pressure\", \"rain\", \"humidity\", \"wind\"\n",
    "    ]\n",
    "assembler = VectorAssembler(inputCols=feature_list, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"price\")\n",
    "pipeline = Pipeline(stages=[assembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [100, 200, 300])\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15])\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossval = CrossValidator(\n",
    "#     estimator=pipeline,\n",
    "#     estimatorParamMaps=param_grid,\n",
    "#     evaluator=RegressionEvaluator(),\n",
    "#     numFolds=5\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_split = TrainValidationSplit(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=RegressionEvaluator(),\n",
    "    trainRatio=0.8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 12:30:21 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 12:30:37 WARN DAGScheduler: Broadcasting large task binary with size 81.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 12:30:47 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.io.ObjectInputStream$HandleTable$HandleList.add(ObjectInputStream.java:4041)\n",
      "\tat java.io.ObjectInputStream$HandleTable.markDependency(ObjectInputStream.java:3860)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2456)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2454)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2142)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1680)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2454)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2454)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2454)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2454)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2454)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "23/03/19 12:30:47 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 1.0 (TID 1),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.io.ObjectInputStream$HandleTable$HandleList.add(ObjectInputStream.java:4041)\n",
      "\tat java.io.ObjectInputStream$HandleTable.markDependency(ObjectInputStream.java:3860)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2456)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2454)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2142)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1680)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2454)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2454)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2454)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2454)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2454)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "23/03/19 12:30:48 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (192.168.0.102 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.io.ObjectInputStream$HandleTable$HandleList.add(ObjectInputStream.java:4041)\n",
      "\tat java.io.ObjectInputStream$HandleTable.markDependency(ObjectInputStream.java:3860)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2456)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2454)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2142)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1680)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2454)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2454)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2454)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2454)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2454)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2378)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2236)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692)\n",
      "\n",
      "23/03/19 12:30:48 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job\n",
      "23/03/19 12:30:48 ERROR Instrumentation: org.apache.spark.SparkException: Job 1 cancelled because SparkContext was shut down\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1188)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1186)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1186)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2887)\n",
      "\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2784)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$11(SparkContext.scala:2095)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2095)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:660)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1470)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.take(RDD.scala:1443)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:119)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:150)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:134)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:43)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3442, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/sc/vbfmws3s1wd7fy72rbmz34nm0000gn/T/ipykernel_7673/3769581291.py\", line 1, in <module>\n",
      "    model = train_valid_split.fit(trainData)\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/tuning.py\", line 1464, in _fit\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/pool.py\", line 873, in next\n",
      "    raise value\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/tuning.py\", line 1464, in <lambda>\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/util.py\", line 337, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/tuning.py\", line 113, in singleTask\n",
      "    index, model = next(modelIter)\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/base.py\", line 98, in __next__\n",
      "    return index, self.fitSingleModel(index)\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/base.py\", line 156, in fitSingleModel\n",
      "    return estimator.fit(dataset, paramMaps[index])\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/base.py\", line 203, in fit\n",
      "    return self.copy(params)._fit(dataset)\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/pipeline.py\", line 134, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/wrapper.py\", line 383, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/wrapper.py\", line 380, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m train_valid_split\u001b[39m.\u001b[39;49mfit(trainData)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/tuning.py:1464\u001b[0m, in \u001b[0;36mTrainValidationSplit._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m   1463\u001b[0m metrics \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m numModels\n\u001b[0;32m-> 1464\u001b[0m \u001b[39mfor\u001b[39;00m j, metric, subModel \u001b[39min\u001b[39;00m pool\u001b[39m.\u001b[39mimap_unordered(\u001b[39mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m   1465\u001b[0m     metrics[j] \u001b[39m=\u001b[39m metric\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[39mraise\u001b[39;00m value\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[39m=\u001b[39m (\u001b[39mTrue\u001b[39;00m, func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[1;32m    126\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/tuning.py:1464\u001b[0m, in \u001b[0;36mTrainValidationSplit._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m   1463\u001b[0m metrics \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m numModels\n\u001b[0;32m-> 1464\u001b[0m \u001b[39mfor\u001b[39;00m j, metric, subModel \u001b[39min\u001b[39;00m pool\u001b[39m.\u001b[39mimap_unordered(\u001b[39mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m   1465\u001b[0m     metrics[j] \u001b[39m=\u001b[39m metric\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/util.py:337\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m SparkContext\u001b[39m.\u001b[39m_active_spark_context\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc()\u001b[39m.\u001b[39msetLocalProperties(properties)\n\u001b[0;32m--> 337\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/tuning.py:113\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msingleTask\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mint\u001b[39m, \u001b[39mfloat\u001b[39m, Transformer]:\n\u001b[0;32m--> 113\u001b[0m     index, model \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(modelIter)\n\u001b[1;32m    114\u001b[0m     \u001b[39m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[39m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[39m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39m#  all nested stages and evaluators\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/base.py:98\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcounter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m index, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfitSingleModel(index)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/base.py:156\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfitSingleModel\u001b[39m(index: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m M:\n\u001b[0;32m--> 156\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator\u001b[39m.\u001b[39;49mfit(dataset, paramMaps[index])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mif\u001b[39;00m params:\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy(params)\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    204\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39;49mfit(dataset)\n\u001b[1;32m    135\u001b[0m     transformers\u001b[39m.\u001b[39mappend(model)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    384\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2068\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2065\u001b[0m     traceback\u001b[39m.\u001b[39mprint_exc()\n\u001b[1;32m   2066\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2068\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_showtraceback(etype, value, stb)\n\u001b[1;32m   2069\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_pdb:\n\u001b[1;32m   2070\u001b[0m     \u001b[39m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2071\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebugger(force\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/ipykernel/zmqshell.py:550\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    544\u001b[0m sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    545\u001b[0m sys\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    547\u001b[0m exc_content \u001b[39m=\u001b[39m {\n\u001b[1;32m    548\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtraceback\u001b[39m\u001b[39m\"\u001b[39m: stb,\n\u001b[1;32m    549\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mename\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(etype\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m),\n\u001b[0;32m--> 550\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevalue\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39;49m(evalue),\n\u001b[1;32m    551\u001b[0m }\n\u001b[1;32m    553\u001b[0m dh \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayhook\n\u001b[1;32m    554\u001b[0m \u001b[39m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[39m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__str__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_exception\u001b[39m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[39m=\u001b[39m gateway_client\u001b[39m.\u001b[39;49msend_command(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexception_cmd)\n\u001b[1;32m    472\u001b[0m     return_value \u001b[39m=\u001b[39m get_return_value(answer, gateway_client, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[39m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[39m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[39m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_command\u001b[39m(\u001b[39mself\u001b[39m, command, retry\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[39m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[39m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m connection\u001b[39m.\u001b[39msocket \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_new_connection\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[39m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_parameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_property, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[39m.\u001b[39;49mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context\u001b[39m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket, server_hostname\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49mconnect((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_address, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39mmakefile(\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_connected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/daviyokogawa/.pyenv/versions/3.10.6/envs/spark_studies/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "model = train_valid_split.fit(trainData)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_studies",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
